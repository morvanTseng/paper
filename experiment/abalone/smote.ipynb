{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# 直接导入Smote是不行的，必须将路径加入才可以\n",
    "sys.path.append(\"C:Users\\perma\\PycharmProjects\\paper\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from algorithm.SMOTE import Smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Sex\", \"Length\", \"Diameter\", \"Height\", \"Whole\", \"Shucked\", \"Viscera\", \"Shell\", \"class\"]\n",
    "major_class = list(range(1, 14))\n",
    "data_path = \"C:\\\\Users\\\\perma\\\\PycharmProjects\\\\paper\\\\data\\\\abalone.data\"\n",
    "\n",
    "table = pd.read_table(filepath_or_buffer=data_path,\n",
    "                      header=None,\n",
    "                      index_col=None,\n",
    "                      names=names,\n",
    "                      sep=\",\"\n",
    "                      )\n",
    "label = []\n",
    "for l in table[\"class\"]:\n",
    "    if l in major_class:\n",
    "        label.append(0.0)\n",
    "    else:\n",
    "        label.append(1.0)\n",
    "dummies = pd.get_dummies(data=table, prefix=[\"Sex\"], columns=[\"Sex\"])\n",
    "attributes = [\"Sex_F\", \"Sex_I\", \"Sex_M\", \"Length\", \"Diameter\", \"Height\", \"Whole\", \"Shucked\", \"Viscera\", \"Shell\"]    \n",
    "data = dummies[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "dc = DecisionTreeClassifier()\n",
    "svm = SVC()\n",
    "lg = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "algorithm = {\"knn\": knn, \"tree\": dc, \"svm\": svm, \"lgs\": lg, \"nb\": nb}\n",
    "alg_params = {\"knn\": {\"n_neighbors\": 7}, \n",
    "              \"tree\": {}, \n",
    "              \"svm\": {}, \n",
    "              \"lgs\": {}, \n",
    "              \"nb\": {}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn  :  gmean  :  0.7504449668191557-----------------knn  :  fscore  :  0.21884566492182794-----------------\n\ntree  :  gmean  :  0.6417807755386297-----------------tree  :  fscore  :  0.19371478231622322-----------------\n\nsvm  :  gmean  :  0.7953933344309189-----------------svm  :  fscore  :  0.23302021345215418-----------------\n\nlgs  :  gmean  :  0.8016360129072083-----------------lgs  :  fscore  :  0.24366952339428594-----------------\n\nnb  :  gmean  :  0.650675054362151-----------------nb  :  fscore  :  0.14990715285969874-----------------\n\n"
     ]
    }
   ],
   "source": [
    "# 将少数类和多数类分开提取出来\n",
    "major = []\n",
    "major_label = []\n",
    "minor = []\n",
    "minor_label = []\n",
    "for label_index in range(len(label)):\n",
    "    if label[label_index] == 1.0:\n",
    "        minor.append(data.iloc[label_index, :])\n",
    "        minor_label.append(label[label_index])\n",
    "    else:\n",
    "        major.append(data.iloc[label_index, :])\n",
    "        major_label.append(label[label_index])\n",
    "# 定义gmean函数\n",
    "def g_mean(ground_truth, prediction):\n",
    "    matrix = confusion_matrix(y_true=ground_truth, y_pred=prediction)\n",
    "    tpr = matrix[1, 1] / (matrix[1, 1] + matrix[1, 0])\n",
    "    tpn = matrix[0, 0] / (matrix[0, 0] + matrix[0, 1])\n",
    "    return np.sqrt(tpr * tpn)\n",
    "result = dict([(key, {\"gmean\": list(), \"fscore\": list()}) for key in algorithm.keys()])\n",
    "# 实验次数\n",
    "n = 100\n",
    "for i in range(n):\n",
    "    for key in algorithm.keys():\n",
    "        alg = algorithm[key]\n",
    "        alg.set_params(**alg_params[key])\n",
    "        major_train, major_test, \\\n",
    "        major_label_train, major_label_test \\\n",
    "            = train_test_split(major, major_label, shuffle=True, test_size=0.25)\n",
    "        minor_train, minor_test, \\\n",
    "        minor_label_train, minor_label_test \\\n",
    "            = train_test_split(minor, minor_label, shuffle=True, test_size=0.25)\n",
    "        data_train = np.concatenate((major_train, minor_train), axis=0)\n",
    "        label_train = np.concatenate((major_label_train, minor_label_train), axis=0)\n",
    "        data_test = np.concatenate((major_test, minor_test), axis=0)\n",
    "        label_test = np.concatenate((major_label_test, minor_label_test), axis=0)\n",
    "        # smote生成合成样本\n",
    "        minor_samples = []\n",
    "        for label_index in range(len(label_train)):\n",
    "            if label_train[label_index] == 1.0:\n",
    "                minor_samples.append(data_train[label_index])\n",
    "        smote = Smote(minor_samples, 700, 3)\n",
    "        smote.over_sampling()\n",
    "        syntheticed_samples = smote.synthetic\n",
    "        syntheticed_labels = len(syntheticed_samples) * [1.0]\n",
    "        syntheticed_train = np.concatenate((data_train, syntheticed_samples), axis=0)\n",
    "        syntheticed_train_label = np.concatenate((label_train, syntheticed_labels), axis=0)\n",
    "        alg.fit(X=syntheticed_train, y=syntheticed_train_label)\n",
    "        prediction = alg.predict(X=data_test)\n",
    "        gmean = g_mean(label_test, prediction)\n",
    "        fscore = f1_score(y_true=label_test, y_pred=prediction) / 2\n",
    "        result[key][\"gmean\"].append(gmean)\n",
    "        result[key][\"fscore\"].append(fscore)\n",
    "new_line = 0       \n",
    "for alg_key in result.keys():\n",
    "    new_line += 1\n",
    "    alg = result[alg_key]\n",
    "    for score_key in alg.keys():\n",
    "        mean_score = np.mean(alg[score_key])\n",
    "        print(alg_key, \" : \", score_key, \" : \", mean_score, end=\"-----------------\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn  :  gmean  :  0.7504449668191557-----------------knn  :  fscore  :  0.21884566492182794-----------------\n",
    "# \n",
    "# tree  :  gmean  :  0.6417807755386297-----------------tree  :  fscore  :  0.19371478231622322-----------------\n",
    "# \n",
    "# svm  :  gmean  :  0.7953933344309189-----------------svm  :  fscore  :  0.23302021345215418-----------------\n",
    "# \n",
    "# lgs  :  gmean  :  0.8016360129072083-----------------lgs  :  fscore  :  0.24366952339428594-----------------\n",
    "# \n",
    "# nb  :  gmean  :  0.650675054362151-----------------nb  :  fscore  :  0.14990715285969874-----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
